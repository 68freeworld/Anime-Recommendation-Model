{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c91af38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # For converting text data into TF-IDF vectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # For computing cosine similarity between vectors\n",
    "from scipy.spatial.distance import pdist, squareform  # For pairwise distance computations and converting to a square matrix\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "# pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.max_rows', 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc6e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anime_df = pd.read_csv('AnimeNEW.csv')\n",
    "anime_df=pd.read_csv(\"anime_meta.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2b264fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anime_name</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>Genres</th>\n",
       "      <th>Score</th>\n",
       "      <th>Synopsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cowboy Bebop</td>\n",
       "      <td>1</td>\n",
       "      <td>Action, Adventure, Comedy, Drama, Sci-Fi, Space</td>\n",
       "      <td>8.78</td>\n",
       "      <td>In the year 2071, humanity has colonized sever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cowboy Bebop: Tengoku no Tobira</td>\n",
       "      <td>5</td>\n",
       "      <td>Action, Drama, Mystery, Sci-Fi, Space</td>\n",
       "      <td>8.39</td>\n",
       "      <td>other day, another bounty—such is the life of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trigun</td>\n",
       "      <td>6</td>\n",
       "      <td>Action, Sci-Fi, Adventure, Comedy, Drama, Shounen</td>\n",
       "      <td>8.24</td>\n",
       "      <td>Vash the Stampede is the man with a $$60,000,0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Witch Hunter Robin</td>\n",
       "      <td>7</td>\n",
       "      <td>Action, Mystery, Police, Supernatural, Drama, ...</td>\n",
       "      <td>7.27</td>\n",
       "      <td>ches are individuals with special powers like ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bouken Ou Beet</td>\n",
       "      <td>8</td>\n",
       "      <td>Adventure, Fantasy, Shounen, Supernatural</td>\n",
       "      <td>6.98</td>\n",
       "      <td>It is the dark century and the people are suff...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        anime_name  anime_id  \\\n",
       "0                     Cowboy Bebop         1   \n",
       "1  Cowboy Bebop: Tengoku no Tobira         5   \n",
       "2                           Trigun         6   \n",
       "3               Witch Hunter Robin         7   \n",
       "4                   Bouken Ou Beet         8   \n",
       "\n",
       "                                              Genres Score  \\\n",
       "0    Action, Adventure, Comedy, Drama, Sci-Fi, Space  8.78   \n",
       "1              Action, Drama, Mystery, Sci-Fi, Space  8.39   \n",
       "2  Action, Sci-Fi, Adventure, Comedy, Drama, Shounen  8.24   \n",
       "3  Action, Mystery, Police, Supernatural, Drama, ...  7.27   \n",
       "4          Adventure, Fantasy, Shounen, Supernatural  6.98   \n",
       "\n",
       "                                            Synopsis  \n",
       "0  In the year 2071, humanity has colonized sever...  \n",
       "1  other day, another bounty—such is the life of ...  \n",
       "2  Vash the Stampede is the man with a $$60,000,0...  \n",
       "3  ches are individuals with special powers like ...  \n",
       "4  It is the dark century and the people are suff...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anime_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be3fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_by_jaccard(\n",
    "    title,\n",
    "    anime_df,\n",
    "    top_n=5,\n",
    "    type='Genres',  # 'Genres'\n",
    "    precomputed=None  # None, DataFrame \n",
    "):\n",
    "    \"\"\"\n",
    "    recommend_by_jaccard is a function that recommends similar anime using Jaccard similarity\n",
    "    based on either Genres or Themes.\n",
    "\n",
    "    :param title: str\n",
    "        The anime title (anime_name) to base recommendations on.\n",
    "\n",
    "    :param anime_df: pd.DataFrame\n",
    "        DataFrame containing at least 'anime_name' and the selected feature column ('Genres' or 'Themes').\n",
    "\n",
    "    :param top_n: int\n",
    "        Number of top similar results to return.\n",
    "\n",
    "    :param type: str\n",
    "        The feature to compute similarity on; must be either 'Genres' or 'Themes'.\n",
    "\n",
    "    :param precomputed: np.ndarray or None\n",
    "        Optional precomputed Jaccard distance array to avoid recalculating distances.\n",
    "\n",
    "    :return: dict\n",
    "        A dictionary with:\n",
    "            { \"top\": pd.Series } — the top-N most similar anime and their similarity scores.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Check if title exists in the dataset\n",
    "    if title not in anime_df['anime_name'].values:\n",
    "        raise ValueError(f\"'{title}' not found in dataset.\")\n",
    "\n",
    "    def compute_jaccard(df, col):\n",
    "        cross_tab = pd.crosstab(df['anime_name'], df[col])\n",
    "        distances = pdist(cross_tab.values, metric='jaccard')\n",
    "        similarity = 1 - squareform(distances)\n",
    "        return pd.DataFrame(similarity, index=cross_tab.index, columns=cross_tab.index)\n",
    "    \n",
    "    def compute_jaccard_array(df, col, arr):\n",
    "        cross_tab = pd.crosstab(df['anime_name'], df[col])\n",
    "        distances = arr\n",
    "        similarity = 1 - squareform(distances)\n",
    "        return pd.DataFrame(similarity, index=cross_tab.index, columns=cross_tab.index)\n",
    "        \n",
    "    df_jaccard = anime_df[[\"anime_name\", type]]\n",
    "    df_filtered = df_jaccard[(df_jaccard[type] != 'Unknown') & (df_jaccard['anime_name'] != 'Unknown')]\n",
    "\n",
    "    sim = precomputed\n",
    "    if sim is None:\n",
    "        sim = compute_jaccard(df_filtered, type)\n",
    "        if title not in sim.index:\n",
    "            raise ValueError(f\"'{title}' not found in similarity data.\")\n",
    "        top = sim.loc[title].sort_values(ascending=False)[1:top_n+1]\n",
    "        return {\"top\": top}\n",
    "        \n",
    "    else:\n",
    "        sim_df = compute_jaccard_array(df_filtered, type, sim)\n",
    "\n",
    "        if title not in sim_df.index:\n",
    "            raise ValueError(f\"'{title}' not found in similarity data.\")\n",
    "        top = sim_df.loc[title].sort_values(ascending=False)[1:top_n+1]\n",
    "        return {\"top\": top}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f149a67",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'John Wax' not found in dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m res = \u001b[43mrecommend_by_jaccard\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mJohn Wax\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manime_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mThemes\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombine\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mrecommend_by_jaccard\u001b[39m\u001b[34m(title, anime_df, top_n, type, combine, precomputed)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Check if title exists in the dataset\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m title \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m anime_df[\u001b[33m'\u001b[39m\u001b[33mEnglish\u001b[39m\u001b[33m'\u001b[39m].values:\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not found in dataset.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_jaccard\u001b[39m(df, col):\n\u001b[32m     51\u001b[39m     cross_tab = pd.crosstab(df[\u001b[33m'\u001b[39m\u001b[33mEnglish\u001b[39m\u001b[33m'\u001b[39m], df[col])\n",
      "\u001b[31mValueError\u001b[39m: 'John Wax' not found in dataset."
     ]
    }
   ],
   "source": [
    "res = recommend_by_jaccard(\"John Wax\", anime_df, type='Themes', combine=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53bd7e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dropJthemes.pkl\", \"rb\") as f:\n",
    "    d = pickle.load(f)\n",
    "with open(\"dropJgenres.pkl\", \"rb\") as f:\n",
    "    e = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "949b0ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ress = recommend_by_jaccard(\"One Punch Man\", anime_df, type='Themes', combine=False, precomputed=d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0fe0c96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'top': English\n",
       " Samurai Flamenco                   1.0\n",
       " Nanako SOS                         1.0\n",
       " One Punch Man Specials             1.0\n",
       " One Punch Man Season 2 Specials    1.0\n",
       " GJ8man \"Highlights\"                1.0\n",
       " Name: One Punch Man, dtype: float64}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1f4a860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'top': English\n",
       " Samurai Flamenco                   1.0\n",
       " Nanako SOS                         1.0\n",
       " One Punch Man Specials             1.0\n",
       " One Punch Man Season 2 Specials    1.0\n",
       " GJ8man \"Highlights\"                1.0\n",
       " Name: One Punch Man, dtype: float64}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03167a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "spot = {'genre': e, 'theme': d}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d356fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "resss = recommend_by_jaccard(\"One Punch Man\", anime_df, top_n=30, combine=True, precomputed=spot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "142e2672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'common': ['One Punch Man Specials',\n",
       "  'One Punch Man 3',\n",
       "  'One Punch Man Season 2',\n",
       "  'One Punch Man',\n",
       "  'One Punch Man 2nd Season Commemorative Special']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ee9ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_similar_anime(title, anime_df, top_n, precomputed=None):\n",
    "    \"\"\"\n",
    "    recommend_similar_anime is a function that recommends the most similar anime \n",
    "    using TF-IDF cosine similarity based on the Synopsis field.\n",
    "\n",
    "    :param title: str\n",
    "        The anime title (anime_name) to base recommendations on.\n",
    "\n",
    "    :param anime_df: pd.DataFrame\n",
    "        DataFrame containing at least 'anime_name' and 'Synopsis' columns.\n",
    "\n",
    "    :param top_n: int\n",
    "        The number of top similar anime to return (excluding the anime itself).\n",
    "\n",
    "    :param precomputed: np.ndarray or None\n",
    "        Optional precomputed cosine similarity matrix. If provided, it will be used instead of recomputing.\n",
    "\n",
    "    :return: dict\n",
    "        A dictionary with:\n",
    "            { \"top\": pd.Series } — the top-N most similar anime and their similarity scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    if title not in anime_df['anime_name'].values:\n",
    "        raise ValueError(f\"'{title}' not found in dataset.\")\n",
    "    \n",
    "    df_content = anime_df[[\"anime_name\", \"Synopsis\"]]\n",
    "    df_content = df_content[(df_content['Synopsis'] != 'Unknown') & (df_content['anime_name'] != 'Unknown')]\n",
    "    df_content = df_content.dropna(subset=['Synopsis', 'anime_name'])\n",
    "\n",
    "    # Vectorize the synopsis\n",
    "    vectorizer = TfidfVectorizer(min_df=2, max_df=0.7, stop_words='english')\n",
    "    vectorized_data = vectorizer.fit_transform(df_content['Synopsis'])\n",
    "    \n",
    "    tfidf_df = pd.DataFrame(\n",
    "        vectorized_data.toarray(),\n",
    "        columns=vectorizer.get_feature_names_out()\n",
    "    )\n",
    "    tfidf_df.index = df_content['anime_name']\n",
    "\n",
    "    if precomputed is not None:\n",
    "        cosine_similarity_df = pd.DataFrame(\n",
    "            precomputed,\n",
    "            index=tfidf_df.index,\n",
    "            columns=tfidf_df.index\n",
    "        )\n",
    "        cosine_similarity_series = cosine_similarity_df.loc[title]\n",
    "        ordered_similarities = cosine_similarity_series.sort_values(ascending=False)\n",
    "        return {\"top\": ordered_similarities}\n",
    "    \n",
    "    else:\n",
    "        # Compute similarity only between the target anime and all others\n",
    "        target_vector = tfidf_df.loc[title].values.reshape(1, -1)\n",
    "        other_vectors = tfidf_df.drop(title, axis=0)\n",
    "\n",
    "        similarities = cosine_similarity(target_vector, other_vectors.values)[0]\n",
    "        similar_titles = other_vectors.index\n",
    "\n",
    "        result_series = pd.Series(similarities, index=similar_titles)\n",
    "        top_similar = result_series.sort_values(ascending=False)[:top_n]\n",
    "\n",
    "        return {\"top\": top_similar}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15a8d91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'top': anime_name\n",
       " One Punch Man: Road to Hero                                                                   0.232880\n",
       " Urawa no Usagi-chan                                                                           0.209826\n",
       " One Punch Man 2nd Season                                                                      0.181862\n",
       " The Four Seasons                                                                              0.170357\n",
       " Kankou Taisen Saitama: Sakuya no Tatakai                                                      0.154468\n",
       " Love Live! School Idol Project: μ's →NEXT LoveLive! 2014 - Endless Parade Makuai Drama        0.138506\n",
       " Love Live! School Idol Project: μ's →NEXT LoveLive! 2014 - Endless Parade Encore Animation    0.137069\n",
       " One Punch Man Specials                                                                        0.134100\n",
       " One Punch Man 2nd Season Specials                                                             0.127306\n",
       " Kumo no Gakkou                                                                                0.126693\n",
       " dtype: float64}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anime_df=pd.read_csv(\"anime_meta.csv\")\n",
    "recommend_similar_anime(\"One Punch Man\", anime_df, 10, precomputed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ae5a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to load\n",
    "# with open(\"coSim_ESdrop.pkl\", \"rb\") as f:\n",
    "#     cosine_similarity_array = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed602401",
   "metadata": {},
   "source": [
    "<h1>USER BASED CF<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12b498eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def uCF(target_user, user_movie2rating, movie2user, user2movie, top_n=10, k=25, min_common=5):\n",
    "    \"\"\"\n",
    "    uCF (User-based Collaborative Filtering) generates top-N movie recommendations for a given target user \n",
    "    using the Pearson correlation similarity between users.\n",
    "\n",
    "    :param target_user: The ID of the user for whom recommendations are to be generated.\n",
    "    :param user_movie2rating: Dictionary mapping (user, movie) tuples to their corresponding rating values.\n",
    "    :param movie2user: Dictionary mapping each movie to the list of users who rated it.\n",
    "    :param user2movie: Dictionary mapping each user to the list of movies they have rated.\n",
    "    :param top_n: Number of recommendations to return (default is 10).\n",
    "    :param k: Number of most similar users (neighbors) to consider for prediction (default is 25).\n",
    "    :param min_common: Minimum number of common movies required to compute similarity (default is 5).\n",
    "    \n",
    "    :return: A list of (movie_id, predicted_rating) tuples representing the top-N recommended movies.\n",
    "    \"\"\"\n",
    "    # Compute global mean ratings for each user\n",
    "    global_means = {}\n",
    "    for user, movies in user2movie.items():\n",
    "        ratings = [user_movie2rating[(user, movie)] for movie in movies if (user, movie) in user_movie2rating]\n",
    "        global_means[user] = np.round(np.mean(ratings), 2) if ratings else 0.0\n",
    "\n",
    "    movies_target = set(user2movie[target_user])\n",
    "    mu_target = global_means[target_user]\n",
    "\n",
    "    similarity_scores = {}\n",
    "    for other_user in user2movie:\n",
    "        if other_user == target_user:\n",
    "            continue\n",
    "        movies_other = set(user2movie[other_user])\n",
    "        common_movies = movies_target.intersection(movies_other)\n",
    "        if len(common_movies) >= min_common:\n",
    "            mu_other = global_means[other_user]\n",
    "            numerator = 0.0\n",
    "            denom_target = 0.0\n",
    "            denom_other = 0.0\n",
    "            for movie in common_movies:\n",
    "                rt = user_movie2rating[(target_user, movie)]\n",
    "                ro = user_movie2rating[(other_user, movie)]\n",
    "                diff_t = rt - mu_target\n",
    "                diff_o = ro - mu_other\n",
    "                numerator += diff_t * diff_o\n",
    "                denom_target += diff_t ** 2\n",
    "                denom_other += diff_o ** 2\n",
    "            if denom_target == 0 or denom_other == 0:\n",
    "                similarity = 0.0\n",
    "            else:\n",
    "                similarity = numerator / (math.sqrt(denom_target) * math.sqrt(denom_other))\n",
    "        else:\n",
    "            similarity = 0.0\n",
    "        similarity_scores[other_user] = similarity\n",
    "\n",
    "    user1_movies = set(user2movie[target_user])\n",
    "    candidate_movies = []\n",
    "    for movie in movie2user:\n",
    "        if movie not in user1_movies:\n",
    "            candidate_movies.append(movie)\n",
    "\n",
    "    predicted_ratings = {}\n",
    "    for movie in candidate_movies:\n",
    "        # Get the list of users who have rated the candidate movie\n",
    "        raters = movie2user[movie]\n",
    "\n",
    "        # Create a list to store tuples of (similarity, rating) for each neighbor\n",
    "        sim_rating_pairs = []\n",
    "\n",
    "        # For each user who rated the movie, if they are not the target user, get the similarity and rating\n",
    "        for user in raters:\n",
    "            if user != target_user:\n",
    "                sim = similarity_scores[user]\n",
    "                if sim != 0:\n",
    "                    rating = user_movie2rating[(user, movie)]\n",
    "                    sim_rating_pairs.append((sim, rating, user))\n",
    "\n",
    "        # Sort the (similarity, rating) pairs in descending order based on similarity\n",
    "        sim_rating_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "        # Select the top k similar users\n",
    "        top_pairs = sim_rating_pairs[:k]\n",
    "\n",
    "        # Compute the weighted rating: weighted sum of ratings divided by sum of similarities\n",
    "        numerator = 0.0\n",
    "        denominator = 0.0\n",
    "        for sim, rating, user in top_pairs:\n",
    "            mu_other = global_means[user]\n",
    "            numerator += sim * (rating - mu_other)\n",
    "            denominator += abs(sim)\n",
    "\n",
    "        # If the denominator is nonzero, compute the predicted rating; otherwise, set it to 0\n",
    "        if denominator != 0:\n",
    "            predicted_rating = mu_target + (numerator / denominator)\n",
    "            predicted_ratings[movie] = np.round(predicted_rating, 1)\n",
    "        else:\n",
    "            predicted_ratings[movie] = 0\n",
    "\n",
    "    recommendations = sorted(predicted_ratings.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d39439cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.read_csv(\"ratings.csv\")\n",
    "user2movie = dff.groupby('user_id')['anime_id'].apply(list).to_dict()\n",
    "movie2user = dff.groupby('anime_id')['user_id'].apply(list).to_dict()\n",
    "user_movie = zip(dff['user_id'], dff['anime_id'])\n",
    "user_movie_rating = zip(user_movie, dff['rating'])\n",
    "user_movie2rating = dict(user_movie_rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eec27fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = uCF(0, user_movie2rating, movie2user, user2movie, min_common=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "74c6ed67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oz no Mahoutsukai no Koutsuu Anzen no Tabi: 10.2\n",
      "Fullmetal Alchemist: Brotherhood: 9.9\n",
      "Niji no Kakehashi: 9.8\n",
      "Neko wa Ikite Iru: 9.7\n",
      "Nana Moon: 9.7\n",
      "Nyanpara no Nakama-tachi: 9.7\n",
      "Nijiiro no Fushigina Ishi: 9.7\n",
      "Ginga Eiyuu Densetsu: 9.6\n",
      "Gintama: 9.6\n",
      "Uchuu Kyoudai: 9.6\n"
     ]
    }
   ],
   "source": [
    "# anime_df['anime_id'] = anime_df['anime_id'].astype(int)\n",
    "# id2name = dict(zip(anime_df['anime_id'], anime_df['anime_name']))\n",
    "\n",
    "dff['anime_id'] = dff['anime_id'].astype(int)\n",
    "id2name = dict(zip(dff['anime_id'], dff['anime_name']))\n",
    "\n",
    "# Now your recommendations will match\n",
    "for anime_id, score in rec:\n",
    "    name = id2name.get(anime_id, f\"[Missing name for ID {anime_id}]\")\n",
    "    print(f\"{name}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8b3e472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(30932, 10.2),\n",
       " (5114, 9.9),\n",
       " (31004, 9.8),\n",
       " (31017, 9.7),\n",
       " (33583, 9.7),\n",
       " (36312, 9.7),\n",
       " (36324, 9.7),\n",
       " (820, 9.6),\n",
       " (918, 9.6),\n",
       " (12431, 9.6)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4b6e64",
   "metadata": {},
   "source": [
    "<h1>Evaluation<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26c744c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start here\n",
    "training_percentage = 0.8\n",
    "training_num_datapoints = int(training_percentage * dff.shape[0])\n",
    "\n",
    "rating_training_df = dff.iloc[:training_num_datapoints]\n",
    "rating_test_df = dff.iloc[training_num_datapoints:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eae7807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_dictionaries(ratings_data_df, dataset='training'):\n",
    "    \"\"\"\n",
    "    create_data_dictionaries is a function that creates dictionaries for user-movie interactions and ratings.\n",
    "\n",
    "    :param ratings_data_df: is a dataFrame containing 'user_id', 'movie_id', and 'rating' columns.\n",
    "    :param dataset: is the type of dataset ('training' or other). For 'training', user-movie mappings are created; \n",
    "    otherwise, they are set as empty dictionaries.\n",
    "    :return: a tuple containing user2movie (dict) which maps each user_id to a list of movie_ids, movie2user (dict) which maps \n",
    "    each movie_id to a list of user_ids, and user_movie2rating (dict) which maps (user_id, movie_id) pairs to their corresponding rating.\n",
    "    \"\"\"\n",
    "    if dataset.lower() == 'training':\n",
    "        user2movie = ratings_data_df.groupby('user_id')['anime_id'].apply(list).to_dict()\n",
    "        movie2user = ratings_data_df.groupby('anime_id')['user_id'].apply(list).to_dict()\n",
    "    else:\n",
    "        user2movie = {}\n",
    "        movie2user = {}\n",
    "    \n",
    "    user_movie = zip(ratings_data_df['user_id'], ratings_data_df['anime_id'])\n",
    "    user_movie_rating = zip(user_movie, ratings_data_df['rating'])\n",
    "    user_movie2rating = dict(user_movie_rating)\n",
    "\n",
    "    return user2movie, movie2user, user_movie2rating\n",
    "\n",
    "def compute_user_average(user2movie, user_movie2rating):\n",
    "    \"\"\"\n",
    "    compute_user_average is a function that calculates the average rating for each user in the dataset.\n",
    "    :param user2movie: a dictionary that maps each user to a list of movie_ids.\n",
    "    :param user_movie2rating: a dictionary that maps each (user, movie) pair to a rating.\n",
    "    :return: a dictionary containing the average rating for each user where the key is the user and the\n",
    "    value is the average rating.\n",
    "    \"\"\"\n",
    "    user_avg = {}\n",
    "    for user, movies in user2movie.items():\n",
    "        ratings = [user_movie2rating[(user, movie)] for movie in movies]\n",
    "        user_avg[user] = np.mean(ratings)\n",
    "    return user_avg\n",
    "\n",
    "def calculate_pearson_similarity(user1, user2, user2movie, user_movie2rating, user_avg, min_common):\n",
    "    \"\"\"\n",
    "    calculate_pearson_similarity is a function that calculates the Pearson similarity between user1 and \n",
    "    user2 based on their common rated movies. If the number of common movies is less than min_common, \n",
    "    similarity is set to 0.\n",
    "    :param user1: the id of user1\n",
    "    :param user2: the id of user2\n",
    "    :param user2movie: a dictionary that maps each user to a list of movie_ids.\n",
    "    :param user_movie2rating: a dictionary that maps each (user, movie) pair to a rating.\n",
    "    :param user_avg: a dictionary of the average ratings for each user.\n",
    "    :param min_common: the required minimum number of common movies between user1 and user2.\n",
    "    :return: the pearson correlation similarity between user1 and user2.\n",
    "    \"\"\"\n",
    "    user1_movies = set(user2movie[user1])\n",
    "    user2_movies = set(user2movie[user2])\n",
    "    common_movies = user1_movies.intersection(user2_movies)\n",
    "    \n",
    "    if len(common_movies) < min_common:\n",
    "        return 0  # not enough common items\n",
    "\n",
    "    numerator = 0\n",
    "    denominator_user1 = 0\n",
    "    denominator_user2 = 0\n",
    "    for movie in common_movies:\n",
    "        rating_user1 = user_movie2rating[(user1, movie)]\n",
    "        rating_user2 = user_movie2rating[(user2, movie)]\n",
    "        \n",
    "        user1_deviation = rating_user1 - user_avg[user1]\n",
    "        user2_deviation = rating_user2 - user_avg[user2]\n",
    "        \n",
    "        numerator += user1_deviation * user2_deviation\n",
    "        denominator_user1 += user1_deviation ** 2\n",
    "        denominator_user2 += user2_deviation ** 2\n",
    "\n",
    "    if denominator_user1 == 0 or denominator_user2 == 0:\n",
    "        return 0\n",
    "    \n",
    "    return numerator / (np.sqrt(denominator_user1) * np.sqrt(denominator_user2))\n",
    "\n",
    "def compute_similarity_matrix(user2movie, user_movie2rating, user_avg, min_common):\n",
    "    \"\"\"\n",
    "    compute_similarity_matrix is a function that precomputes the similarity for each pair of users in \n",
    "    the training set and saves the similarity scores in a dictionary.\n",
    "    :param user2movie: a dictionary that maps each user to a list of movie_ids.\n",
    "    :param user_movie2rating: a dictionary that maps each (user, movie) pair to a rating.\n",
    "    :param user_avg: a dictionary of the average ratings for each user.\n",
    "    :param min_common: the required minimum number of common movies between a pair of movies to be eligible\n",
    "    for similarity calculation.\n",
    "    :return: a nested dictionary where the key is a user and the value is a dictionary of the similarity \n",
    "    score between the key user and all the other users.\n",
    "    \"\"\"\n",
    "    similarity_matrix = {}\n",
    "    all_users = list(user2movie.keys())\n",
    "    print(len(all_users))\n",
    "    for i, user1 in enumerate(all_users):\n",
    "        if user1 not in similarity_matrix:\n",
    "            similarity_matrix[user1] = {}\n",
    "    \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('{} users processed'.format(i + 1))\n",
    "\n",
    "        for j in range(i + 1, len(all_users)):\n",
    "            user2 = all_users[j]\n",
    "            pearson_similarity = calculate_pearson_similarity(user1, user2, user2movie, user_movie2rating, user_avg, min_common)\n",
    "            similarity_matrix[user1][user2] = pearson_similarity\n",
    "            \n",
    "            if user2 not in similarity_matrix:\n",
    "                similarity_matrix[user2] = {}\n",
    "            similarity_matrix[user2][user1] = pearson_similarity\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "def predict_rating(user, movie, user2movie, user_movie2rating, user_avg, min_common, k_value):\n",
    "    \"\"\"\n",
    "    predict_rating is a function that predicts the rating user \"user\" would give to movie \"movie\".\n",
    "    :param user: user_id\n",
    "    :pram movie: movie_id\n",
    "    :param user2movie: a dictionary that maps each user to a list of movie_ids.\n",
    "    :param user_movie2rating: a dictionary that maps each (user, movie) pair to a rating.\n",
    "    :param user_avg: a dictionary of the average ratings for each user.\n",
    "    :param min_common: the required minimum number of common movies between a pair of movies to be eligible\n",
    "    for similarity calculation.\n",
    "    :param k_value: the number of nearest neighbors to be considered when predicting the user's rating.\n",
    "    :return: the predicted rating.\n",
    "    \"\"\"\n",
    "    ## find candidate neighbors who rated the movie\n",
    "    candidates = [other_user for other_user in movie2user[movie] if other_user != user] \n",
    "    \n",
    "    similarities = []\n",
    "    for other_user in candidates:\n",
    "        pearson_similarity = similarity_matrix[user][other_user]\n",
    "        if pearson_similarity != 0:\n",
    "            similarities.append((other_user, pearson_similarity))\n",
    "    \n",
    "    if not similarities:\n",
    "        return user_avg[user]\n",
    "    \n",
    "    ## sort neighbors by the absolute similarity in descending order and select top k neighbors.\n",
    "    similarities = sorted(similarities, key=lambda x: abs(x[1]), reverse=True)\n",
    "    top_neighbors = similarities[:k_value]\n",
    "    \n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for neighbor, similarity in top_neighbors:\n",
    "        rating_neighbor = user_movie2rating[(neighbor, movie)]\n",
    "        numerator += similarity * (rating_neighbor - user_avg[neighbor])\n",
    "        denominator += abs(similarity)\n",
    "    \n",
    "    if denominator == 0:\n",
    "        return user_avg[user]\n",
    "    \n",
    "    predicted_rating = user_avg[user] + numerator / denominator\n",
    "    return predicted_rating\n",
    "\n",
    "def evaluate_model_rmse(user2movie, user_movie2rating, user_avg, dataset_rating, similarity_matrix, k_value):\n",
    "    \"\"\"\n",
    "    evaluate_model_rmse is a function that uses RMSE evaluation metric to evaluate the performance of the \n",
    "    recommendation system.\n",
    "    :param user2movie: a dictionary that maps each user to a list of movie_ids.\n",
    "    :param user_movie2rating: a dictionary that maps each (user, movie) pair to a rating.\n",
    "    :param user_avg: a dictionary of the average ratings for each user.\n",
    "    :param dataset_rating: a dictionary containing a mapping from (user, movie) pair to a rating.\n",
    "    :similarity_matrix: the precomputed matrix of similarity scores between all pairs of users.\n",
    "    :param k_value: the number of nearest neighbors to be considered when predicting the user's rating.\n",
    "    :return: the RMSE score.\n",
    "    \"\"\"\n",
    "    \n",
    "    squared_errors = []\n",
    "    for ind, ((user, movie), actual_rating) in enumerate(dataset_rating.items()):\n",
    "        predicted_rating = predict_rating(user, movie, user2movie, user_movie2rating, user_avg, similarity_matrix, k_value)\n",
    "        squared_errors.append((actual_rating - predicted_rating) ** 2)\n",
    "        if (ind + 1) % 100 == 0:\n",
    "            print('{} ratings processed'.format(ind + 1))\n",
    "    mse = np.mean(squared_errors)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "feb80c4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m user2movie, movie2user, user_movie2rating = \u001b[43mcreate_data_dictionaries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrating_training_df\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m _, _, user_movie2rating_test = create_data_dictionaries(\n\u001b[32m      6\u001b[39m     rating_test_df, dataset=\u001b[33m'\u001b[39m\u001b[33mTest\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m user_avg = compute_user_average(user2movie, user_movie2rating)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mcreate_data_dictionaries\u001b[39m\u001b[34m(ratings_data_df, dataset)\u001b[39m\n\u001b[32m     18\u001b[39m user_movie = \u001b[38;5;28mzip\u001b[39m(ratings_data_df[\u001b[33m'\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m'\u001b[39m], ratings_data_df[\u001b[33m'\u001b[39m\u001b[33manime_id\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     19\u001b[39m user_movie_rating = \u001b[38;5;28mzip\u001b[39m(user_movie, ratings_data_df[\u001b[33m'\u001b[39m\u001b[33mrating\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m user_movie2rating = \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43muser_movie_rating\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m user2movie, movie2user, user_movie2rating\n",
      "\u001b[31mMemoryError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "user2movie, movie2user, user_movie2rating = create_data_dictionaries(\n",
    "    rating_training_df\n",
    ")\n",
    "\n",
    "_, _, user_movie2rating_test = create_data_dictionaries(\n",
    "    rating_test_df, dataset='Test'\n",
    ")\n",
    "\n",
    "user_avg = compute_user_average(user2movie, user_movie2rating)\n",
    "\n",
    "min_common_value = 5\n",
    "\n",
    "similarity_matrix = compute_similarity_matrix(user2movie, user_movie2rating, user_avg, min_common_value)\n",
    "\n",
    "k_value = 10\n",
    "\n",
    "train_rmse = evaluate_model_rmse(user2movie, user_movie2rating, user_avg, user_movie2rating, min_common_value, k_value)\n",
    "\n",
    "test_rmse = evaluate_model_rmse(user2movie, user_movie2rating, user_avg, user_movie2rating_test, min_common_value, k_value)\n",
    "\n",
    "print('k: {}, min_common: {}, Train RMSE: {}, Test RMSE: {}'.format(k_value, min_common_value, np.round(train_rmse, 3), np.round(test_rmse, 3)))\n",
    "\n",
    "k_values = [5, 10, 20, 30, 40, 50]\n",
    "\n",
    "results = {}\n",
    "for k_value in k_values:\n",
    "    train_rmse = evaluate_model_rmse(user2movie, user_movie2rating, user_avg, user_movie2rating, min_common_value, k_value)\n",
    "    test_rmse = evaluate_model_rmse(user2movie, user_movie2rating, user_avg, user_movie2rating_test, min_common_value, k_value)\n",
    "    results[(k_value, min_common_value)] = [train_rmse, test_rmse]\n",
    "\n",
    "print(\n",
    "    'k: {}, min_common: {}, Train RMSE: {}, Test RMSE: {}'.format(\n",
    "        k_value, min_common_value, np.round(train_rmse, 3), np.round(test_rmse, 3)\n",
    "    )\n",
    ")\n",
    "\n",
    "best_params = min(results.items(), key=lambda x: x[1][1])\n",
    "best_k, best_min_common = best_params[0]\n",
    "best_train_rmse, best_test_rmse = best_params[1]\n",
    "\n",
    "print(\"Best hyperparameters:\")\n",
    "print(\"k: {}, min_common: {} with Train RMSE: {}, Test RMSE: {}\".format(best_k, best_min_common, np.round(best_train_rmse, 3), np.round(best_test_rmse, 3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8b26f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
